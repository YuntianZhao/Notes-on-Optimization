\setchapterstyle{kao}
\setchapterpreamble[u]{\margintoc}
\chapter{Convexity}
\labch{convexity}

\section{Convex geometry}

\subsection{Convex sets}

\begin{definition}
A set $C \subseteq \R^n$ is convex if for all $a, b \in C$ and $0 \le \theta \le 1$, we have $\theta a + (1-\theta) b \in C$.
\end{definition}

If two points belong to a convex set, then all points that are on the line between these two points belong to this convex set.

\begin{definition}[Convex combination]
Let $\{ a^1, \dots, a^p \} \subset \R^n$.
A convex combination of $a^1, \dots a^p$ is $\sum _{k=1}^{p} \theta_{k} a^k$ for some $\theta_k \ge 0$ and $\sum _{k=1}^{p} \theta_k = 1$.
\end{definition}

The convex combination can be regarded as a linear combination with restraints on coefficients $\theta_k$.

\begin{definition}[Convex polytope]
The convex polytope generated by $a^1, \dots, a^p$ is $\langle a^1, \dots, a^p \rangle = \left\{ \sum _{k=1}^{p} \theta_{k} a^k : \theta_k \ge 0, \sum _{k=1}^{p} \theta_k = 1 \right\}$
\end{definition}

A convex polytope is a set of all possible convex combinations.

\begin{theorem}[Caratheodory's Theorem]
Let $b$ belong to the convex polytope $\langle a^1, \dots, a^p \rangle \subset \R^n$.
Then we can write $b = \sum _{k=1}^{n+1} \theta_k a^{j_k}$ where $1 \le j_1 < \cdots < j_{n+1} \le p, \theta_k \ge 0, \sum _{k=1}^{n+1} \theta_k = 1$.
\end{theorem}

\begin{proof}
Since $b \in \langle a^1, \dots, a^p \rangle$, there exists a solution $x \in \R^p$ of 
\begin{align}
A x = \begin{bmatrix}
b \\
1 \\
\end{bmatrix},
x \ge 0
\end{align}
for the $(n+1) \times p$ matrix 
\begin{align}
A = \begin{bmatrix}
a^1 & a^2 & \cdots & a^p \\
1   & 1   & \cdots & 1   \\
\end{bmatrix}.
\end{align}
And according to \refthm{LP_basic_solution}, there exists a basic solution $x^* \in \R^p$ of 
\begin{align}
A x^* = \begin{bmatrix}
b \\
1 \\
\end{bmatrix},
x^* \ge 0.
\end{align}
This means that $x^*$ has at most $n+1$ nonzero entries $\{ x_{j_1}, \dots, x_{j_{n+1}} \}$ corresponding to the independent rows of $A$.
Then relabel $\theta_k = x_{j_k}$ for $k = 1, \dots, n+1$.
\end{proof}

\subsection{Separating hyperplanes}

We discuss now the geometry of convex sets and of hyperplanes. 

\begin{lemma}
\lablemma{separating_hyperplane}
Let $C$ be a nonempty, closed, convex, subset of $\R^n$ and suppose $0 \notin C$. 
Then there exists a unique point $x_0 \in C$ such that $|x_0| = \min \ \left\{ |x| : x \in C \right\}$.
Furthermore, $0 \le x_0 ^T (x-x_0)$ for all $x \in C$.
\end{lemma}

\begin{proof}
Step 1:
Let $\delta = \inf \ \{ |x| : x \in C \} \ge 0$.
Select $\{ x^k \}_{k=1}^{\infty} \subset C$ with $\delta = \lim _{k \to \infty} |x^k|$.
According to Bolzano-Weierstrass Theorem, there exists a convergent subsequence $\lim _{j \to \infty} x^{k_j} = x_0$.
Consequently, $|x_0| = \lim _{j \to \infty} x^{k_j} = \delta$.
Since $C$ is closed, $x_0 \in C$.
And since $0 \notin C$, $\delta > 0$.

Step 2:
We claim that $x_0$ is a unique point in $C$ with $|x_0| = \delta$.
Too see this, suppose $x_1 \in C$  also satisfies $|x_1| = \delta$. Then $\frac{x_0 + x_1}{2} \in C$ and $\left|\frac{x_0 + x_1}{2}\right| \ge \delta$.
However,
\begin{align}
\underbrace{|x_0 - x_1|^2}_{\ge 0} + \underbrace{|x_0 + x_1|^2}_{\ge 4 \delta^2} = \underbrace{2 (|x_0|^2 + |x_1|^2)}_{=4 \delta^2}.
\end{align}
Thus we must have $x_0 = x_1$.

Step 3:
Let $x$ be any point in $C$.
Then for $0 < \theta < 1$, $(1-\theta) x_0 + \theta x \in C$.
Therefore,
\begin{align}
|x_0|^2 & \le |(1-\theta) x_0 + \theta x|^2 \\
        & = |x_0 + \theta (x-x_0)|^2 \\
        & = |x_0|^2 + 2 \theta x_0^T (x-x_0) + \theta ^2 |x-x_0|^2 \\
0       & \le 2 x_0^T (x-x_0) + \theta |x-x_0|^2
\end{align}
Sending $\theta \to 0$, we have $0 \le x_0 ^T (x-x_0)$.
\end{proof}

\begin{definition}[Hyperplane]
Let $a \in \R^n, b \in \R$.
An expression of the form $a^T x + b = 0$ determines a hyperplane in $\R^n$.
The hyperplane is $\{ x : \in \R^n : a^T x + b = 0 \}$.
\end{definition}

It is an $(n-1)$-dimensional affine subspace and passes through the origin if and only if $b = 0$.

\begin{definition}
Let $S_1, S_2$ be two subsets of $\R^n$.
The hyperplane $a^T x + b = 0$ separates $S_1$ and $S_2$ if $a^T x + b \ge 0$ for all $x \in S_1$ and $a^T x + b \le 0$ for all $x \in S_2$.
The hyperplane $a^T x + b = 0$ strictly separates $S_1$ and $S_2$ if $a^T x + b > 0$ for all $x \in S_1$ and $a^T x + b < 0$ for all $x \in S_2$.
\end{definition}

\begin{theorem}[Separating Hyperplane Theorem]
\labthm{separating_hyperplane}
Let $C$ be convex, closed and non-empty, and suppose $e \notin C$.
Then there exists a hyperplane $a^T x + b$ that strictly separates $C$ and $\{ e \}$.
\end{theorem}

\begin{remark}
It is important for subsequent applications that in Separating Hyperplane Theorem, we do not require that $C$ be bounded.
\end{remark}

\begin{proof}
Step 1:
Upon shifting the coordinates if necessary, we may assume $e = 0$.
According to \reflemma{separating_hyperplane}, there exists $x_0 \in C$ such that $0 < \delta = |x_0| = \min \ \{ |x| : x \in C \}$.
We construct the separating hyperplane by finding a perpendicular bisector of the segment $\overline{e x_0}$.
Let $m = x_0 / 2$ and $a = x_0 / \delta$, then $a^T (x - m) = 0$ is the separating hyperplane.
Or equivalently, $a^T x + b = 0$ where $b = -a^T m$.

Step 2:
For $e=0$, $a^T (0 - m) = -\frac{|x_0|^2}{2 \delta} < 0$.

For $C$, taking $x \in C$, we have
\begin{align}
a^T (x-m) 
& = \frac{1}{\delta} x_0^T (x- x_0/2) \\
& = \frac{1}{\delta} \left( \underbrace{x_0^T (x- x_0)}_{\ge 0} + \underbrace{|x_0|^2/2}_{>0} \right) \\
& > 0
\end{align}
Thus the hyperplane $a^T x + b$ strictly separates $\{ e \}$ and $C$.
\end{proof}

\subsection{Dual convex sets}

As a first application of separating hyperplanes, we discuss next a geometric form of convex duality.

\begin{definition}[Polar dual]
\labdef{polar_dual}
Let $C \subset \R^n$ be closed and convex, with $0 \in C$.
Its polar dual is the set $C^0 = \{ y \in R^n: x^T y \le 1, \forall x \in C \}$.
\end{definition}

\begin{theorem}[Dual convex sets]
\labthm{dual_convex_sets}
Under \refdef{polar_dual}:
\begin{enumerate}
    \item $C^0$ is closed, convex, with $0 \in C^0$.
    \item We have the duality assertion $(C^0)^0 = C$.
\end{enumerate}
\end{theorem}

\begin{proof}
Step 1:
To see $C^0$ is closed, we can equivalently show that $(C^0)^c = \{ y \in \R^n : x^T y > 1, \exists x \in C \}$ is open.
To check openness, we must show that for any $y_0 \in (C^0)^c$, there exists a $r$-ball such that $B_r(y_0) \subset (C^0)^c$.
Let us take a fixed $y_0 \in (C^0)^c$, there exists $x_0 \in C$ with $x_0^T y_0 = \delta > 1$.
Let $r > 0$ satisfy $|x_0| r < \delta - 1$, $y_r \in B_r(y_0)$.
And denote $\Delta y_r = y_r - y_0$.
Clearly, $|\Delta y_r| \le r$ and
\begin{align}
x_0^T y_r &= x_0^T y_0 + x_0^T \Delta y_r \\
& \ge \delta - |x_0^T \Delta y_r| \\
& \ge \delta - |x_0| |\Delta y_r| \\
& \ge \delta - |x_0| r > 1.
\end{align}
So that for all $y_0 \in (C^0)^c$, there exists a $r$-ball such that $B_r(y_0) \subset (C^0)^c$.
Thus $(C^0)^c$ is open and $C^0$ is closed.

Step 2:
$C^0$ is convex.
For any $a, b \in C^0$, for all $x \in C$, we have $x^T a \le 1$ and $x^T b \le 1$.
Consequently, for all $0 \le \theta \le 1$, $x^T (\theta a + (1-\theta) b) \le 1$.
Thus $\theta a + (1-\theta) b \in C^0$ as well.

Step 3:
For any $x \in C$, $x^T 0 = 0 \le 1$.
Therefore $0 \in C^0$.

Step 4:
Note that $(C^0)^0 = \{ z \in \R^n : y^T z \le 1, \forall y \in C^0 \}$.
Let $x \in C$. Then $y^T x \le 1$ for all $y \in C^0$ and thus $x \in (C^0)^0$. 
Consequently $C \subseteq (C^0)^0$.

If $z \in (C^0)^0 - C$, then since $C$ is closed, \refthm{separating_hyperplane} says there exists $a \in \R ^n, b \in \R$ such that 
\begin{enumerate}
    \item $a^T z + b < 0$,
    \item $a^T x + b > 0$ for all $x \in C$.
\end{enumerate}
Since $0 \in C$, 2 implies $b > 0$.
So if we write $y = -\frac{a}{b}$, 2 says also that $y^T x < 1$ for all $x \in C$.
Hence $y \in C^0$.
Since $z \in (C^0)^0$, it follows that $y^T z \le 1$; and therefore $a^T z + b \ge 0$.
But this contradicts 1.
Thus $(C^0)^0-C$ is empty and hence $(C^0)^0 = C$.
\end{proof}

\subsection{Farkas alternative}

Our next goal is the Farkas alternative, a statement about solving vector inequalities.
This turns out to have a surprising geometric interpretation involving separating hyperplanes for certain convex cones.

\begin{definition}[Finite cone]
\labdef{finite_cone}
Let $\{ a^1, \dots, a^n \} \subset \R^m$.
The finite cone generated by $\{ a^1, \dots, a^n \}$ is the set $C = \{ \sum _{i=1}^{n} x_i a^i : x_i \ge 0 \}$.
\end{definition}

Observe that $b \in C$ precisely when we can solve $Ax =b, x \ge 0$, when $A = [a^1 \dots a^n]$ is the $m \times n$ matrix whose columns are $\{ a^1, \dots, a^n \}$.

\begin{definition}[Basic cone]
\labdef{basic_cone}
If $\{ a^1, \dots, a^k \}$ are independent, we call the finite cone they generate a basic cone.
\end{definition}

\begin{lemma}
Suppose $\{ a^1, \dots, a^n \}$ generate the finite cone $C$. 
Let $C_1, \dots, C_q$ be the basic cones generated by all linearly independent subsets of $\{ a^1, \dots, a^n \}$.
Then 
\begin{align}
C = \bigcup _{i=1} ^{q} C_i.
\end{align}
\end{lemma}

\begin{proof}
Obviously, $C_i \subseteq C$, so that $\cup_{i=1}^{q} C_i \subseteq C$.

Now select $b \in C = \{ A x : x \ge 0 \}$. 
There exists a solution of $A x = b, x \ge 0$.
And according to \refthm{LP_basic_solution}, there in fact exists a basic solution $x^*$ such that $A x^* = b, x^* \ge 0$.
This means that the columns $\{a^{j_1}, \dots, a^{j_m} \}$ of $A$ corresponding to the nonzero entries of $x^*$ are independent. 
SO $b$ belongs to the basic cone generated by $\{a^{j_1}, \dots, a^{j_m} \}$ and thus $b \in \cup_{i=1}^{q} C_i$.
This is true for all $b \in C$, consequently $C \subseteq \cup_{i=1}^{q} C_i$.
\end{proof}

\begin{theorem}
\labthm{finite_cone_convex_closed}
Let $C$ be the finite cone generated by $\{ a^1, \dots, a^n \} \subset \R^m$.
Then $C$ is convex and closed.
\end{theorem}

\begin{proof}
Step 1:
We need to show that $C$ is convex.
Let $b^1, b^2 \in C, 0 \le \theta \le 1$.
Then there exist $x^1, x^2$ such that $b^1 = A x^1, x^1 \ge 0$ and $b^2 = A x^2, x^2 \ge 0$.
Therefore $(1-\theta)b^1 + \theta b^2 = A \left( (1-\theta) x^1 + \theta x^2 \right)$ for $(1-\theta) x^1 + \theta x^2 \ge 0$.
Thus $(1-\theta)b^1 + \theta b^2 \in C$, and so $C$ is convex.

Step 2:
Let $C_i$ be a basic cone generated by an independent set$\{ a^{j_1}, \dots, a^{j_l} \} \subseteq \{ a^1, \dots, a^n \}$.
Assume $\{ b^k \}_{k=1}^{\infty} \subset C_i$, with $\lim _{k \to \infty} b^k = b^0$.
We claim that $b^0 \in C_i$ and this will show that $C_i$ is closed.

First, let us write $B = \{ j_1, \dots, j_l \}$.
Since the vectors $\{ a^j : j \in B \}$ are independent, if $u = [u_{j_1} \dots u_{j_l}]^T \in \R^l$ and $\sum _{j \in B} u_j a^j = 0$, it follows that $u = 0$.
Therefore for all $u \in \R^l$ with $|u| = 1, \sum _{j \in B} u_j a^j \neq 0$.
Hence the Extreme Value Theorem implies that there exists $\epsilon > 0$ such that $\min \left\{ |\sum _{j \in B} u_j a^j| : |u| = 1 \right\} = \epsilon > 0$.
Thus if $v \in \R^l$, 
\begin{align}
|\sum _{j \in B} v_j a^j| \ge \epsilon |v|. \labeq{proof_convex_closed_eq_1}
\end{align}
We now turn to the proof of $b^0 \in C_i$.
Observe that we can write $b^k = A x^k$, where $x^k \ge 0, x^k = [0 \ x_{j_1}^k \ 0 \dots x_{j_l}^k \ 0]^T$.
Then $b^k = \sum _{j \in B} x_j^k a^j$, and therefore \arefeq{proof_convex_closed_eq_1} implies 
\begin{align}
|x^k| \le \frac{1}{\epsilon} |b^k|
\end{align}
for $k=1, \dots$
The sequence $\{ x^k \}_{k=1}^{\infty}$ is therefore bounded, and so we can apply the Bolzano-Weierstrass Theorem to extract a convergent subsequence:
\begin{align}
\lim _{j \to \infty} x^{k_j} = x^0.
\end{align}
Then $x^0 \ge 0$ and $A x^0 = \lim _{j \to \infty} A x^{k_j} = \lim _{j \to \infty} b^{k_j} = b^0$.
Furthermore $x_j^0 = 0$ except possibly for the indices $j \in B$.
Hence $b^0 \in C_i$.

Step 3:
So each basic cone $C_i$ is closed.
The finite union of closed sets is closed, and hence $C = \cup _{i=1}^{q} C_i$ is closed.
\end{proof}

In view of the previous theorem, we can apply the Separating Hyperplane
Theorem to a finite cone. 
This has the following major payoff:

\begin{theorem}[Farkas alternative]
\labthm{farkas_alternative}
Let $A$ be an $m \times n$ matrix, $b \in \R^m$.
Then either 
\begin{enumerate}
    \item $A x = b, x \ge 0$ has a solution $x \in \R^n$; or
    \item $A^T y \ge 0, b^T y < 0$ has a solution $y \in \R^m$,
\end{enumerate}
but not both.
\end{theorem}

\begin{proof}
Step 1:
Assume $x$ solves 1, y solves 2.
Then $0 \le x^T (A^T y) = (Ax)^T y = b^T y < 0$ which is a contradiction.
So 1 and 2 cannot both be true.

Step 2:
Suppose 1 fails.
We will show that then 2 must hold.
Now the failure of 1 means $b \in C = \{ Ax : x \ge 0\}$.
Since $C$ is a finite cone, \refthm{finite_cone_convex_closed} tells us  that $C$ is closed and convex.
Then the Separating Hyperplane Theorem (\refthm{separating_hyperplane}) asserts that there exist $a \in \R^m, c \in \R$ such that 
\begin{align}
a^T z + c > 0 \quad (z \in C) \labeq{proof_farkas_alternative_eq_1}
\end{align}
and 
\begin{align}
a^T b + c < 0. \labeq{proof_farkas_alternative_eq_2}
\end{align}
Let $x \ge 0, \mu \ge 0$.
Set $z = \mu A x = A (\mu x) \in C$.
Then according to \arefeq{proof_farkas_alternative_eq_1}, $a^T (\mu A x) + C > 0$.
Dividing by $\mu > 0$ and letting $\mu \to \infty$, we see that $a^T Ax \ge 0$.
So $(A^T a)^T x \ge 0$ for all $x \ge 0$.
Thus $A^T a \ge 0$.
Let $y = a$, then $A^T y \ge 0$.
Put $z = 0$ in \arefeq{proof_farkas_alternative_eq_1}, to deduce that $c > 0$.
Then \arefeq{proof_farkas_alternative_eq_2} says
\begin{align}
b^T y = b^T a < -c < 0.
\end{align}
\end{proof}

\section{Convex functions}

A convex function is a real-valued function such that the region above its graph is a convex set.
Convex functions therefore inherit many useful properties from convex sets.

\subsection{Convex functions of one variable}

\begin{definition}
A function $f: \R \to \R$ is called convex if 
\begin{align}
f(\theta x_1 + (1-\theta) x_2) \le \theta f(x_1) + (1-\theta) f(x_2) \tag{$C_1$} \labeq{C1}
\end{align}
for all $x_1, x_2 \in \R, 0 \le \theta \le 1$.

A function $g: \R \to \R$ is called concave if $-g$ is convex.
\end{definition}

If $f$ is convex, then for all points $x_1, x_2$, the graph of $f$ lies below the line segment  connecting $[x_1 \ f(x)1)]^T$ and
$[x_2 \ f(x_2)]^T$.

It is easy to see that $f:\R \to \R$ is a convex function if and only if its epigraph
\begin{align}
E = \left\{ 
\begin{bmatrix}
x \\
y \\
\end{bmatrix} :
y \ge f(x), x \in \R
\right\} \subset \R^2
\end{align}
is a convex set.

\begin{remark}
It follows by induction that if $f: \R \to \R$ is convex, then
\begin{align}
f (\sum_{i} \theta_i x_i) \le \sum_{i} \theta_i f(x_i)
\end{align}
for all $x_i \in \R$, $\theta_i \ge 0$ and $\sum_{i} \theta_i = 1$.
\end{remark}

\begin{theorem}[Equivalent characterizations of convexity]
If $f:\R \to \R$ is continuously differentiable, then $f$ is convex if and only if 
\begin{align}
f(x_1) + f'(x_1) (x_2 - x_1) \le f(x_2) \tag{$C_2$} \labeq{C2}
\end{align}
for all $x_1, x_2 \in \R$.

If $f$ is twice continuously differentiable, then $f$ is convex if and only if
\begin{align}
f''(x) \ge 0 \tag{$C_3$} \labeq{C3}
\end{align}
for all $x \in \R$.
\end{theorem}

\begin{proof}
Step 1:
Assume $f$ is continuously differentiable, and let us show \arefeq{C1} holds if and only if \arefeq{C2} holds.
So, suppose \arefeq{C1}.
For $\theta > 0$, 
\begin{align}
f(\theta x_2 + (1-\theta) x_1) \le \theta f(x_2) + (1-\theta) f(x_1) \\
\frac{f(x_1 + \theta (x_2 - x_1)) - f(x_1)}{\theta} \le f(x_2) - f(x_1)
\end{align}
Let $\theta \to 0$, to deduce that $f'(x_1) (x_2 - x_1) \le f(x_2) - f(x_1)$ which is \arefeq{C1}.

Now assume \arefeq{C2}.
Then if $w = \theta x_1 + (1-\theta) x_2$, we have
\begin{align}
f(x_1) & \ge f(w) + f'(w) (x_1-w) \\
f(x_2) & \ge f(w) + f'(w) (x_2-w).
\end{align}
So
\begin{align}
\theta f(x_1) + (1-\theta) f(x_2) & \ge f(w) + f'(w) \underbrace{( \theta (x_1 - w) + (1-\theta) (x_2 - w) )}_{=0} \\
& = f(\theta x_1 + (1-\theta) x_2 ).
\end{align}
So we get \arefeq{C1}.

Step 2:

\end{proof}

The condition \arefeq{C3} is especially convenient for checking if a given function is convex or not.
But the graphs of convex functions can have corners, and so convex functions need not be twice or even once continuously differentiable.
However, they are always continuous.

\begin{theorem}[Convex functions are continuous]
If $f:\R \to \R$ is convex, then $f$ is continuous.
\end{theorem}

\begin{proof}
Step 1:
First we show that $f$ is bounded on each interval $[a, b]$ of finite length.
For each $a \le x \le b$, we can write $x = \theta a + (1-\theta) b$ where $0 \le \theta \le 1$.
Therefore,
\begin{align}
f(x) \le \theta f(a) + (1-\theta) f(b) \le \max \ \{ 
|f(a)|, |f(b)| \}.
\end{align}
Now if $\frac{a+b}{2} \le x \le b$, then $\frac{a+b}{2} = \theta x + (1-\theta) a$ for $\theta = \frac{b-a}{2(x-a)}$.
Then $1/2 \le \theta \le 1$ and convexity implies $f(\frac{a+b}{2}) \le \theta f(x) + (1-\theta) f(a)$.
Hence
\begin{align}
f(x) \ge \frac{1}{\theta} \left( f(\frac{a+b}{2}) - (1-\theta) f(a) \right) \ge -2 \left( |f(\frac{a+b}{2})| + |f(a)| \right)
\end{align}
since $\frac{1}{\theta} \le 2$.
Likewise, if $a \le x \le \frac{a+b}{2}$, we also have
\begin{align}
f(x) \ge -2 \left( |f(\frac{a+b}{2})| + |f(a)| \right).
\end{align}
Therefore
\begin{align}
\sup_{[a, b]} |f(x)| \le 4 \max \ \{ |f(a)|, |f(\frac{a+b}{2})|, |f(b)| \} \le \infty.
\end{align}

Step 2:
Then we prove the continuity for $f$ on $[0, 1]$.
Let $0 \le x < y \le 1$.
Then $y = \theta x + (1-\theta) 2$ for $\theta = \frac{2-y}{2-x}$.
Thus $f(y) \le \theta f(x) + (1-\theta) f(2)$, and so
\begin{align}
f(y) - f(x) & \le (1-\theta)(f(2) -f(x)) \\
& = \underbrace{\frac{1}{2-x}}_{\in [1/2, 1]} (y-x) \underbrace{(f(2) - f(x))}_{\le 2 \sup_{[0, 2]} |f|} \\
& \le 2 |y-x| \sup _{[0, 2]} |f(x)|
\end{align}
Similarly, we have $x = \theta y + (1-\theta) (-2)$ for $\theta = \frac{y+2}{x+2}$.
Then
\begin{align}
f(x) - f(y) \le 2 |y-x| \sup _{[-2, 1]} |f(x)|.
\end{align}
Together we have
\begin{align}
|f(y) - f(x)| \le 2 |y-x| \sup _{[-2, 2]} |f(x)|.
\end{align}
Since $\sup _{[-2, 2]} |f(x)|$ is bounded, this inequality implies $f$ is continuous on $[0, 1]$.

Step 3:
We will finish by shifting $f$ on any finite interval $[a, b]$ to $\hat{f}$ that defined on $[0, 1]$.
Let $\hat{f}(x) = f(\frac{x-a}{b-a})$.
The continuity for $\hat{f}$ is also applied for $f$.
If $f$ is continuous, then $\hat{f}(x) = f(ax + b)$ is also continuous.
For all $\epsilon > 0$, there exists a $r_0 > 0$ such that with given $x_0$ for all $y \in B_{r_0}(x_0)$, $|f(y) - f(x_0)| < \epsilon$.
So if we have $x_0 = a x + b$ and $r = r_0 / a$, then within $y \in B_r(x)$, 
\begin{align}
|\hat{f}(y) -\hat{f}(x)| = |f(a y+b) - f(a x+b)| < \epsilon.
\end{align}
So that $\hat{f}$ is continuous.
\end{proof}

\subsection{Convex functions of more variables}

\begin{definition}
A function $f: \R^n \to \R$ is called convex if 
\begin{align}
f(\theta x_1 + (1-\theta) x_2) \le \theta f(x_1) + (1-\theta) f(x_2) \tag{$C_1'$} \labeq{C1p}
\end{align}
for all $x_1, x_2 \in \R, 0 \le \theta \le 1$.

A function $g: \R \to \R$ is called concave if $-g$ is convex.
\end{definition}

If $f$ is convex, then for all points $x_1, x_2$, the graph of $f$ lies below the line segment  connecting $[x_1 \ f(x)1)]^T$ and
$[x_2 \ f(x_2)]^T$.

It is easy to see that $f:\R^n \to \R$ is a convex function if and only if its epigraph
\begin{align}
E = \left\{ 
\begin{bmatrix}
x \\
y \\
\end{bmatrix} :
y \ge f(x), x \in \R^n
\right\} \subset \R^{n+1}
\end{align}
is a convex set.

\begin{remark}
It follows by induction that if $f: \R^n \to \R$ is convex, then
\begin{align}
f (\sum_{i} \theta_i x_i) \le \sum_{i} \theta_i f(x_i)
\end{align}
for all $x_i \in \R^n$, $\theta_i \ge 0$ and $\sum_{i} \theta_i = 1$.
\end{remark}

\begin{theorem}[Equivalent characterizations of multivariable convexity]
If $f:\R^n \to \R$ is continuously differentiable, then $f$ is convex if and only if 
\begin{align}
f(x_1) + \grad f(x_1)^T (x_2 - x_1) \le f(x_2) \tag{$C_2'$} \labeq{C2p}
\end{align}
for all $x_1, x_2 \in \R^n$.

If $f$ is twice continuously differentiable, then $f$ is convex if and only if
\begin{align}
\grad ^2 f(x) \simeq 0 \tag{$C_3'$} \labeq{C3p}
\end{align}
for all $x \in \R^n$.
\end{theorem}

\begin{theorem}[Multivariable convex functions are continuous]
If $f:\R^n \to \R$ is convex, then $f$ is continuous.
\end{theorem}

\subsection{Subdifferentials}

\begin{definition}[Subdifferential]
Let $f:\R^n \to \R$ be convex.
For each $x_1 \in \R^n$, we define
\begin{align}
\pp f(x_1) = \{ r \in \R^n : f(x_1) + r^T (x_2 - x_1) \le f(x_2), \forall x_2 \in \R^n \}.
\end{align}
This set is called the subdifferential of $f$ at $x_1$.
\end{definition}

\begin{example}
Let $n=1$ and $f(x) = |x|$.
Then
\begin{align}
\pp f(x) = 
\begin{cases}
\{ -1 \}, & x < 0 \\
[-1, 1], & x = 0 \\
\{ 1 \}, & x > 0 \\
\end{cases}
\end{align}
\end{example}

\begin{theorem}[Properties of subdifferential]
\labthm{properties_subdifferential}
Let $f:\R^n \to \R$ be convex.
Then for each $x \in \R^n$, $\pp f(x)$ is a closed, convex, and non-empty set.
\end{theorem}

\begin{proof}
Step 1:
Convexity.
For any $r_1, r_2 \in \pp f(x)$ and $0 \le \theta \le 1$, 
\begin{align}
f(x) + \left[ \theta r_1 + (1-\theta) r_2 \right]^T (x_1 - x) 
&= \theta \left[ f(x) + r_1 (x_1 - x) \right] + (1-\theta) \left[ f(x) + r_2 (x_1 - x) \right] \\
&\le f(x_1)
\end{align}
Thus, $\left[ \theta r_1 + (1-\theta) r_2 \right] \in \pp f(x)$.

Step 2:
Closeness. 
Assume now $\{r_k\}_{k=1}^{\infty} \subset \pp f(x)$ and $r_0 = \lim_{k \to \infty} r_k$.
Then for each $k$ and each $x_1$, $f(x) + r_k^T (x_1 - x) \le f(x_1)$.
Let $k \to \infty$ to deduce that $f(x) + r_0^T (x_1 - x) \le f(x_1)$ for each $x_1 \in \R^n$ and hence $r_0 \in \pp f(x)$.
Consequently, $\pp f(x)$ is closed.

Step 3:
Non-emptiness.
Select any point $x \in \R^n$. 
Will show that $\pp f(x) \neq \emptyset$.
TODO.

\end{proof}

\subsection{Dual convex functions}

For this section, assume $f:\R^n \to \R$ is convex, with
\begin{align}
\lim _{|x| \to \infty} \frac{f(x)}{|x|} = + \infty.
\end{align}
This is called a super-linear growth condition.

\begin{definition}
For $y \in \R^n$, the dual convex function (or Legendre transform) of $f$ is
\begin{align}
f^{*}(y) = \max _{x \in \R^n} \{ x^T y - f(x) \}.
\end{align}
\end{definition}

\begin{example}
Let $f(x) = \frac{x^2}{2}$ for $x \in \R$.
Then 
\begin{align}
f^{*}(y) = \max _{x \in \R} \{ x y - \frac{x^2}{2} \} = \frac{y^2}{2}
\end{align}
\end{example}

\begin{lemma}[Fenchel-Young inequality]
\lablemma{F-Y inequality}
For all $x, y \in \R^n$ we have
\begin{align}
x^T y \le f(x) + f^{*}(y).
\end{align}
\end{lemma}

\begin{theorem}
The function $f^{*}: \R^n \to \R$ is convex.
\end{theorem}

\begin{proof}
\begin{align}
f^{*}(\theta y_1 + (1-\theta) y_2 ) 
&= \max _{x} \left\{ x^T \left( \theta y_1 + (1-\theta) y_2 \right) - f(x) \right\} \\
&= \max _{x} \left\{ \theta \left[ x^T y_1 - f(x) \right] + (1-\theta) \left[ x^T y_2 - f(x) \right] \right\} \\
&\le \theta \max _{x} \left\{ x^T y_1 - f(x) \right\} + (1-\theta) \max _{x} \left\{ x^T y_2 - f(x) \right\} \\
&= \theta f^*(y_1) + (1-\theta) f^*(y_2) 
\end{align}
\end{proof}

\begin{theorem}
$\lim_{|y| \to \infty} \frac{f^{*}(y)}{|y|} = +\infty$.
\end{theorem} 

\begin{proof}
According to \reflemma{F-Y inequality}, $f(x) + f^{*}(y) \ge x^T y $ for all $x, y$. 
Fix $y \neq 0, \mu > 0$ and let $x = \frac{\mu y}{|y|}$.
Then
\begin{align}
f^{*}(y) 
&\ge \left( \frac{\mu y}{|y|} \right)^T y - f(\frac{\mu y}{|y|}) 
&\ge \mu |y| - \max_{x \in B_{\mu}(0)} f(x)
\end{align}
So
\begin{align}
\frac{f^{*}(y)}{|y|} &\ge mu - \frac{1}{|y|} \max_{x \in B_{\mu}(0)} f(x) \\
\lim _{|y| \to \infty} \frac{f^{*}(y)}{|y|} &\ge \mu
\end{align}
for all $\mu > 0$.
\end{proof}

\begin{theorem}
$f^{**} = f$.
\end{theorem}

\begin{proof}
Step 1:
According to \reflemma{F-Y inequality}, $f(x) \ge x^T y - f^{*}(y)$ for all $x, y$.
\begin{align}
f(x) \ge \max_{y} \left\{ x^T y - f^{*}(y) \right\} = f^{**}(x)
\end{align}

Step 2:
\refthm{properties_subdifferential} tells us that $\pp f(x)$ is non-empty.
Select $r \in \pp f(x)$, then $f(z) \ge f(x) + r^T (z- x)$ for all $z \in \R^n$.
Consequently\sidenote{FIRST $=$,WHY???}, 
\begin{align}
r^T x - f(x) = \max_{z} \{ r^T z - f(z) \} = f^*(r)
\end{align}
and so
\begin{align}
f^{**}(x) = \max_{y} \left\{ x^T y - f^{*}(y) \right\} \ge x^T r - f^{*}(r) = f(x)
\end{align}
\end{proof}

\begin{theorem}[Subdifferentials and dual functions]
For all points $x, y \in \R^n$, the following are equivalent.
\begin{enumerate}
    \item $x^T y = f(x) + f^{*}(y)$;
    \item $y \in \pp f(x)$;
    \item $x \in \pp f^{*}(y)$.
\end{enumerate}
\end{theorem}

\begin{proof}
TODO
\end{proof}

\section{Applications}
TODO